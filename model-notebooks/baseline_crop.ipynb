{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras import layers, models, Input, GlobalAveragePooling2D, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Configure MirroredStrategy for multi-GPU training\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download PlantDoc.zip\n",
    "gdown.download(\"https://drive.google.com/uc?id=1GKs5BTRjrjBuVv7UqHow3XaeYZlmipHg\", \"plantdoc.zip\", quiet=False)\n",
    "\n",
    "# Download PlantVillage.zip\n",
    "gdown.download(\"https://drive.google.com/uc?id=1olr9AIq3XK1x3S79PcFcJSd4mG6f8ZLa\", \"plantvillage.zip\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define extraction paths\n",
    "plant_doc_extract_path = '/kaggle/working/plantdoc/plantdoc'\n",
    "plant_village_extract_path = '/kaggle/working/plantvillage/plantvillage'\n",
    "\n",
    "# Extract PlantDoc dataset\n",
    "with zipfile.ZipFile(\"plantdoc.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(plant_doc_extract_path)\n",
    "\n",
    "# Extract PlantVillage dataset\n",
    "with zipfile.ZipFile(\"plantvillage.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(plant_village_extract_path)\n",
    "\n",
    "# Display extracted folders to verify\n",
    "print(\"Extracted PlantDoc dataset files:\", os.listdir(plant_doc_extract_path))\n",
    "print(\"Extracted PlantVillage dataset files:\", os.listdir(plant_village_extract_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths to the CSV files\n",
    "train_csv_path = '/kaggle/input/plant-dataset/PV_train.csv'       # Change to your actual path on Kaggle\n",
    "test_seen_csv_path = '/kaggle/input/plant-dataset/PV_test_seen.csv'\n",
    "test_unseen_csv_path = '/kaggle/input/plant-dataset/PV_test_unseen.csv'\n",
    "plantdoc_unseen_csv_path = '/kaggle/input/plant-dataset/PD_test_unseen.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV files into pandas DataFrames\n",
    "train_df = pd.read_csv(train_csv_path, header=None)\n",
    "test_seen_df = pd.read_csv(test_seen_csv_path, header=None)\n",
    "test_unseen_df = pd.read_csv(test_unseen_csv_path, header=None)\n",
    "plantdoc_unseen_df = pd.read_csv(plantdoc_unseen_csv_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Display the first few rows of each DataFrame to verify\n",
    "print(\"Training Data Sample:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"Seen Testing Data Sample:\")\n",
    "print(test_seen_df.head())\n",
    "\n",
    "print(\"Unseen Testing Data Sample:\")\n",
    "print(test_unseen_df.head())\n",
    "\n",
    "print(\"PlantDoc Unseen Testing Data Sample:\")\n",
    "print(plantdoc_unseen_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Assign column names manually\n",
    "train_df.columns = ['image', 'crop', 'disease']\n",
    "test_seen_df.columns = ['image', 'crop', 'disease']\n",
    "test_unseen_df.columns = ['image', 'crop', 'disease']\n",
    "plantdoc_unseen_df.columns = ['image', 'crop', 'disease']\n",
    "\n",
    "# Convert the 'crop' column to string type\n",
    "train_df['crop'] = train_df['crop'].astype(str)\n",
    "test_seen_df['crop'] = test_seen_df['crop'].astype(str)\n",
    "test_unseen_df['crop'] = test_unseen_df['crop'].astype(str)\n",
    "plantdoc_unseen_df['crop'] = plantdoc_unseen_df['crop'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get unique crops in each dataset\n",
    "train_crops = train_df['crop'].unique()\n",
    "test_seen_crops = test_seen_df['crop'].unique()\n",
    "test_unseen_crops = test_unseen_df['crop'].unique()\n",
    "plantdoc_unseen_crops = plantdoc_unseen_df['crop'].unique()\n",
    "\n",
    "# Print the number of unique crops in each dataset\n",
    "print(f\"Number of unique crops in training dataset: {len(train_crops)}\")\n",
    "print(f\"Number of unique crops in test seen dataset: {len(test_seen_crops)}\")\n",
    "print(f\"Number of unique crops in test unseen dataset: {len(test_unseen_crops)}\")\n",
    "print(f\"Number of unique crops in plantdoc unseen dataset: {len(plantdoc_unseen_crops)}\")\n",
    "\n",
    "# Print unique crops in each dataset for a comparison\n",
    "print(f\"Unique crops in training dataset: {train_crops}\")\n",
    "print(f\"Unique crops in test seen dataset: {test_seen_crops}\")\n",
    "print(f\"Unique crops in test unseen dataset: {test_unseen_crops}\")\n",
    "print(f\"Unique crops in plantdoc unseen dataset: {plantdoc_unseen_crops}\")\n",
    "\n",
    "# Check if the crops in all datasets are the same\n",
    "print(\"Are crops in test seen the same as training crops?\", set(train_crops) == set(test_seen_crops))\n",
    "print(\"Are crops in test unseen the same as training crops?\", set(train_crops) == set(test_unseen_crops))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate label names from training data\n",
    "label_names = [str(crop) for crop in train_crops]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot encode training and test seen labels based on training crops\n",
    "train_labels_df = pd.get_dummies(train_df['crop']).reindex(columns=label_names, fill_value=False).values\n",
    "test_seen_labels_df = pd.get_dummies(test_seen_df['crop']).reindex(columns=label_names, fill_value=False).values\n",
    "\n",
    "# One-hot encode test unseen labels based on training crops\n",
    "# This step ensures that unseen labels have the same structure as training labels\n",
    "test_unseen_labels_df = pd.get_dummies(test_unseen_df['crop']).reindex(columns=label_names, fill_value=False)\n",
    "test_unseen_labels = test_unseen_labels_df.values\n",
    "\n",
    "# One-hot encode PlantDoc unseen labels based on training crops\n",
    "plantdoc_unseen_labels_df = pd.get_dummies(plantdoc_unseen_df['crop']).reindex(columns=label_names, fill_value=False)\n",
    "plantdoc_unseen_labels = plantdoc_unseen_labels_df.values\n",
    "\n",
    "# Verify the shape of each dataset to ensure consistency\n",
    "print(\"Train Labels Shape:\", train_labels.shape)\n",
    "print(\"Test Seen Labels Shape:\", test_seen_labels.shape)\n",
    "print(\"Test Unseen Labels Shape:\", test_unseen_labels.shape)\n",
    "print(\"PlantDoc Unseen Labels Shape:\", plantdoc_unseen_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check the first few rows of each DataFrame to confirm correctness\n",
    "print(\"Train Labels Head:\")\n",
    "print(pd.DataFrame(train_labels_df).head())  \n",
    "\n",
    "print(\"\\nTest Seen Labels Head:\")\n",
    "print(pd.DataFrame(test_seen_labels_df).head()) \n",
    "\n",
    "print(\"\\nTest Unseen Labels Head:\")\n",
    "print(test_unseen_labels_df.head())\n",
    "\n",
    "print(\"\\nPlantdoc Unseen Labels Head:\")\n",
    "print(plantdoc_unseen_labels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Print column names of the training DataFrame\n",
    "print(\"Columns in training DataFrame:\", train_df.columns)\n",
    "\n",
    "# Print column names of the seen testing DataFrame\n",
    "print(\"Columns in seen testing DataFrame:\", test_seen_df.columns)\n",
    "\n",
    "# Print column names of the unseen testing DataFrame\n",
    "print(\"Columns in unseen testing DataFrame:\", test_unseen_df.columns)\n",
    "\n",
    "# Print column names of the unseen testing DataFrame\n",
    "print(\"Columns in plantdoc unseen testing DataFrame:\", plantdoc_unseen_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to update image paths with the full directory prefix\n",
    "def update_image_paths(df, base_path):\n",
    "    df['image'] = df['image'].apply(lambda x: os.path.join(base_path, x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Adjust the path for nested directories\n",
    "correct_base_path_plantvillage = '/kaggle/working/plantvillage/plantvillage/plantvillage'\n",
    "# Adjust the path for nested directories\n",
    "correct_base_path_plantdoc = '/kaggle/working/plantdoc/plantdoc/plantdoc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the path exists\n",
    "if os.path.exists(correct_base_path_plantvillage):\n",
    "    print(\"Path exists. Here are some files or folders in the directory:\\n\")\n",
    "    # List the first few files or folders in the directory\n",
    "    for i, item in enumerate(os.listdir(correct_base_path_plantvillage)):\n",
    "        print(item)\n",
    "        if i >= 9:  # Limit the output to the first 10 items\n",
    "            break\n",
    "else:\n",
    "    print(\"The path does not exist. Please check the path and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the path exists\n",
    "if os.path.exists(correct_base_path_plantdoc):\n",
    "    print(\"Path exists. Here are some files or folders in the directory:\\n\")\n",
    "    # List the first few files or folders in the directory\n",
    "    for i, item in enumerate(os.listdir(correct_base_path_plantdoc)):\n",
    "        print(item)\n",
    "        if i >= 9:  # Limit the output to the first 10 items\n",
    "            break\n",
    "else:\n",
    "    print(\"The path does not exist. Please check the path and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Update paths in the DataFrames\n",
    "train_df = update_image_paths(train_df, correct_base_path_plantvillage)\n",
    "test_seen_df = update_image_paths(test_seen_df, correct_base_path_plantvillage)\n",
    "test_unseen_df = update_image_paths(test_unseen_df, correct_base_path_plantvillage)\n",
    "plantdoc_unseen_df = update_image_paths(plantdoc_unseen_df, correct_base_path_plantdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing files\n",
    "missing_train_files = train_df[~train_df['image'].apply(os.path.exists)]\n",
    "missing_test_seen_files = test_seen_df[~test_seen_df['image'].apply(os.path.exists)]\n",
    "missing_test_unseen_files = test_unseen_df[~test_unseen_df['image'].apply(os.path.exists)]\n",
    "missing_test_field_files = plantdoc_unseen_df[~plantdoc_unseen_df['image'].apply(os.path.exists)]\n",
    "\n",
    "print(f\"Number of missing files in training set: {len(missing_train_files)}\")\n",
    "print(f\"Number of missing files in test seen set: {len(missing_test_seen_files)}\")\n",
    "print(f\"Number of missing files in test unseen set: {len(missing_test_unseen_files)}\")\n",
    "print(f\"Number of missing files in test field set: {len(missing_test_field_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Split the train dataframe into training and validation sets\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.2, stratify=train_df['crop'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create image paths and labels for training and validation sets\n",
    "train_image_paths = train_data['image'].values\n",
    "val_image_paths = val_data['image'].values\n",
    "train_labels = pd.get_dummies(train_data['crop']).values\n",
    "val_labels = pd.get_dummies(val_data['crop']).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare image paths and labels for each test set\n",
    "test_seen_image_paths = test_seen_df['image'].values\n",
    "test_unseen_image_paths = test_unseen_df['image'].values\n",
    "plantdoc_unseen_image_paths = plantdoc_unseen_df['image'].values\n",
    "\n",
    "# Check the shape of image paths and labels for all test datasets\n",
    "print(\"Test Seen - Image Paths Shape:\", test_seen_image_paths.shape)\n",
    "print(\"Test Unseen - Image Paths Shape:\", test_unseen_image_paths.shape)\n",
    "print(\"Test Field - Image Paths Shape:\", plantdoc_unseen_image_paths.shape)\n",
    "\n",
    "print(\"Test Seen Labels Shape:\", test_seen_labels.shape)\n",
    "print(\"Test Unseen Labels Shape:\", test_unseen_labels.shape)\n",
    "print(\"Test Field Labels Shape:\", plantdoc_unseen_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to process the images and labels\n",
    "def process_data(image_path, label, is_training=True):\n",
    "    # Load and preprocess the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [300, 300])\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to apply data augmentation\n",
    "def augmentation(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create training and validation datasets\n",
    "def create_dataset(image_paths, labels, batch_size, is_training=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: (augmentation(process_data(x, y)[0]), y) if is_training else process_data(x, y), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create the datasets\n",
    "train_dataset = create_dataset(train_image_paths, train_labels, batch_size=32, is_training=True)\n",
    "val_dataset = create_dataset(val_image_paths, val_labels, batch_size=32, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_test_dataset(image_paths, labels, batch_size=32):\n",
    "    # Create a dataset without augmentation (testing phase)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(process_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create datasets for seen and unseen test images\n",
    "test_seen_dataset = create_test_dataset(test_seen_image_paths, test_seen_labels, batch_size=32)\n",
    "test_unseen_dataset = create_test_dataset(test_unseen_image_paths, test_unseen_labels, batch_size=32)\n",
    "plantdoc_unseen_dataset = create_test_dataset(plantdoc_unseen_image_paths, plantdoc_unseen_labels, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check dataset shapes\n",
    "for image_batch, label_batch in train_dataset.take(1):\n",
    "    print(\"Training Batch - Image Shape:\", image_batch.shape)\n",
    "    print(\"Training Batch - Label Shape:\", label_batch.shape)\n",
    "\n",
    "for image_batch, label_batch in val_dataset.take(1):\n",
    "    print(\"Validation Batch - Image Shape:\", image_batch.shape)\n",
    "    print(\"Validation Batch - Label Shape:\", label_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define model within strategy scope\n",
    "with strategy.scope():\n",
    "    \n",
    "    # Define the input shape explicitly\n",
    "    input_shape = (300, 300, 3)\n",
    "\n",
    "    # Create an Input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype=tf.float32)\n",
    "\n",
    "    # Load the base EfficientNetB3 model with the specified input shape\n",
    "    base_model = EfficientNetB3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs  # Connect the input layer to EfficientNetB3\n",
    "    )\n",
    "\n",
    "    # Freeze the base model layers\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom layers on top of EfficientNetB3\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(len(train_labels[0]), activation='softmax')(x)\n",
    "\n",
    "    # Create the complete model by specifying the inputs and outputs\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model using the training generator\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the model weights after training\n",
    "model.save_weights('/kaggle/working/efficientnet_b3_baseline.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define an SGD optimizer with momentum\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define an RMSProp optimizer\n",
    "rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # Recreate the model structure, ensuring the input shape and layers match the original\n",
    "    input_shape = (300, 300, 3)\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape, dtype=tf.float32)\n",
    "\n",
    "    # Load the base EfficientNetB3 model\n",
    "    base_model = EfficientNetB3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs\n",
    "    )\n",
    "\n",
    "    # Add custom layers on top of EfficientNetB3\n",
    "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(len(train_labels[0]), activation='softmax')(x)\n",
    "\n",
    "    # Create the complete model by specifying the inputs and outputs\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "\n",
    "    # Load the previously saved weights (matching the current strategy)\n",
    "    model.load_weights('/kaggle/working/efficientnet_b3_baseline.weights.h5')\n",
    "\n",
    "    # Unfreeze some layers for fine-tuning\n",
    "    for layer in base_model.layers[-30:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Compile the fine-tuned model with SGD optimizer\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train the fine-tuned model with SGD\n",
    "fine_tuning_history_sgd = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,  # Fewer epochs for fine-tuning\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model.save('/kaggle/working/fine_tuned_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the initial training history\n",
    "with open('/kaggle/working/initial_training_history.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "# Save the fine-tuning history\n",
    "with open('/kaggle/working/fine_tuning_history.pkl', 'wb') as file:\n",
    "    pickle.dump(fine_tuning_history_sgd.history, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot accuracy and loss curves\n",
    "def plot_accuracy_loss_curves(history):\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Call the plotting function with the history object\n",
    "plot_accuracy_loss_curves(history)\n",
    "plot_accuracy_loss_curves(fine_tuning_history_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_dataset(model, dataset, label_names):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the seen dataset, calculating Top-1, Top-5 accuracy, and generating a confusion matrix.\n",
    "    \"\"\"\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    for image_batch, label_batch in dataset:\n",
    "        # Predict on the batch\n",
    "        batch_predictions = model.predict(image_batch)\n",
    "        \n",
    "        # Store the true labels and predictions\n",
    "        true_labels.extend(np.argmax(label_batch, axis=1))  # Convert one-hot to class indices\n",
    "        predictions.extend(batch_predictions)\n",
    "    \n",
    "    # Convert predictions to numpy array\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate Top-1 and Top-5 accuracy\n",
    "    top1_predictions = np.argmax(predictions, axis=1)\n",
    "    top5_predictions = np.argsort(predictions, axis=1)[:, -5:]  # Get indices of top 5 predictions\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    top1_accuracy = np.mean(top1_predictions == true_labels)\n",
    "    top5_correct = np.any(top5_predictions == true_labels.reshape(-1, 1), axis=1)\n",
    "    top5_accuracy = np.mean(top5_correct)\n",
    "\n",
    "    # Generate confusion matrix and classification report\n",
    "    cm = confusion_matrix(true_labels, top1_predictions)\n",
    "    report = classification_report(true_labels, top1_predictions, target_names=label_names)\n",
    "    \n",
    "    return top1_accuracy, top5_accuracy, cm, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate on seen dataset\n",
    "seen_top1_acc, seen_top5_acc, seen_cm, seen_report = evaluate_dataset(\n",
    "    model, test_seen_dataset, label_names\n",
    ")\n",
    "print(f\"Seen Dataset - Top-1 Accuracy: {seen_top1_acc * 100:.2f}%\")\n",
    "print(f\"Seen Dataset - Top-5 Accuracy: {seen_top5_acc * 100:.2f}%\")\n",
    "print(\"Confusion Matrix (Seen):\\n\", seen_cm)\n",
    "print(\"Classification Report (Seen):\\n\", seen_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Top-1 accuracy on unseen dataset\n",
    "unseen_top1_acc, unseen_top5_acc, _, _ = evaluate_dataset(\n",
    "    model, test_unseen_dataset, label_names\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
